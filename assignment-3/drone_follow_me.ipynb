{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Drone follow me using Kalman Filters\n",
        "\n",
        "Multi-Object Tracking (MOT) is a core visual ability that humans poses to perform kinetic tasks and coordinate other tasks. The AI community has recognized the importance of MOT via a series of [competitions](https://motchallenge.net).\n",
        "\n",
        "In this assignment, the object class is `bicycle` and `car` the ability to track these objects  will be demonstrated using [Kalman Filters](https://en.wikipedia.org/wiki/Kalman_filter).  \n",
        "\n",
        "\n",
        "## Task 1: Setup your development environment and store the test video locally (10 points)\n",
        "\n",
        "Your environment must be docker based and you can use any TF2 or PT2 based docker container compatible with your environment. You can also use colab."
      ],
      "metadata": {
        "id": "KKhXC4if7ETq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Libraries"
      ],
      "metadata": {
        "id": "tCX_7TLi8QPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytube opencv-python psycopg2 ultralytics roboflow"
      ],
      "metadata": {
        "id": "aRAWPNl68Pew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Dependencies"
      ],
      "metadata": {
        "id": "TjXyme528SFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytube import YouTube\n",
        "import os\n",
        "import cv2\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "from google.colab import userdata\n",
        "# google patch for cv2.imshow\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "from roboflow import Roboflow\n",
        "from ultralytics import YOLO"
      ],
      "metadata": {
        "id": "3AJSbH0u8ftk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Videos"
      ],
      "metadata": {
        "id": "suP0MAOL8iNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_videos(video_id_list, output_path=\".\"):\n",
        "    \"\"\"\n",
        "    Download videos from YouTube based on a list of video IDs using PyTube API\n",
        "\n",
        "    Args:\n",
        "        video_id_list (list): A list of YouTube video IDs to download.\n",
        "        output_path (str, optional): The directory where the downloaded videos will be saved. Defaults to ./videos\n",
        "    \"\"\"\n",
        "    for video_id in video_id_list:\n",
        "      try:\n",
        "        yt = YouTube(f\"https://www.youtube.com/watch?v={video_id}\")\n",
        "        video = yt.streams.filter(file_extension='mp4', resolution='360p').first()\n",
        "        video.download(output_path)\n",
        "        print(f\"Video downloaded successfully: {yt.title}\")\n",
        "      except Exception as e:\n",
        "        print(f\"Error downloading video: {e}\")\n",
        "\n",
        "def preprocess_video_from_file(video_path, timestamps = [], sample_rate=1, target_size=(640, 640)):\n",
        "    \"\"\"\n",
        "    Preprocesses frames from a video file.\n",
        "\n",
        "    Parameters:\n",
        "    - video_path (str): Path to the video file.\n",
        "    - sample_rate (int): Frame sampling rate, indicating the frequency of frame sampling (e.g., sample every N frames).\n",
        "    - target_size (tuple): Target size for resizing frames, specified as (height, width).\n",
        "    - timestamps (list): A list to store timestamps corresponding to each sampled frame.\n",
        "\n",
        "    Returns:\n",
        "    - numpy.ndarray: Array of preprocessed frames with shape (num_frames, height, width, channels).\n",
        "      Each frame is resized, converted to RGB, and appended with its corresponding timestamp.\n",
        "\n",
        "    Note:\n",
        "    - The frames are resized to the specified target size.\n",
        "    - Timestamps are saved in a separate array for each sampled frame.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a VideoCapture object\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Check if the video opened successfully\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open video.\")\n",
        "        return None\n",
        "\n",
        "    # Lists to store preprocessed frames\n",
        "    frames = []\n",
        "\n",
        "    # Iterate through the frames\n",
        "    while True:\n",
        "        # Read a frame from the video\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        # Check if the video has ended\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Sample frames\n",
        "        if cap.get(1) % sample_rate == 0:\n",
        "            # Resize the frame\n",
        "            frame = cv2.resize(frame, target_size)\n",
        "\n",
        "            # Convert BGR to RGB\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Normalize the frame\n",
        "            frame = frame / 255.0\n",
        "\n",
        "            # save timestamps to separate array\n",
        "            timestamps.append(cap.get(cv2.CAP_PROP_POS_MSEC))\n",
        "\n",
        "            # Append the preprocessed frame to the list\n",
        "            frames.append(frame)\n",
        "\n",
        "    # Release the VideoCapture object\n",
        "    cap.release()\n",
        "\n",
        "    # Return list of frames converted to a NumPy array\n",
        "    return np.array(frames)"
      ],
      "metadata": {
        "id": "ctgurRWx8lNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Video IDs specified in assignment\n",
        "video_id_list = [\"WeF4wpw7w9k\", \"2NFwY15tRtA\", \"5dRramZVu2Q\"]\n",
        "\n",
        "# Get current directory, define subdirectory for videos, and define path\n",
        "current_directory = os.getcwd()\n",
        "videos_directory = \"videos\"\n",
        "output_path = os.path.join(current_directory, videos_directory)\n",
        "\n",
        "# Join the current directory with the videos directory and download videos\n",
        "download_videos(video_id_list, output_path)\n",
        "\n",
        "# save all mp4 paths to video_paths list\n",
        "video_paths = []\n",
        "for root, dirs, files in os.walk(output_path):\n",
        "  for file in files:\n",
        "    full_path = os.path.join(root, file)\n",
        "    video_paths.append(full_path)\n",
        "\n",
        "# keep in sorted order for consistent IDs in PG instance\n",
        "video_paths.sort()\n",
        "\n",
        "for video_path in video_paths:\n",
        "  print(video_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJeSKzIa_Hgi",
        "outputId": "1a46a7a8-ae2d-4d1e-ffc3-f4825b2d75ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video downloaded successfully: Cyclist and vehicle Tracking - 1\n",
            "Video downloaded successfully: Cyclist and vehicle tracking - 2\n",
            "Video downloaded successfully: Drone Tracking Video\n",
            "/content/videos/Cyclist and vehicle Tracking - 1.mp4\n",
            "/content/videos/Cyclist and vehicle tracking - 2.mp4\n",
            "/content/videos/Drone Tracking Video.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "frames = preprocess_video_from_file(video_paths[0], sample_rate=5)\n",
        "print(frames.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qrah4dLkACip",
        "outputId": "7d98a294-be4a-44fe-d596-e3aaba5197b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(269, 640, 640, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Object Detection (40 points)\n",
        "\n",
        "Perform object detection on the following videos.\n",
        "\n",
        "```{eval-rst}\n",
        ".. youtube:: WeF4wpw7w9k\n",
        "```\n",
        "\n",
        "```{eval-rst}\n",
        ".. youtube:: 2NFwY15tRtA\n",
        "```\n",
        "\n",
        "```{eval-rst}\n",
        ".. youtube:: 5dRramZVu2Q\n",
        "```\n",
        "\n",
        "Split the videos into frames and use an object detector of your choice, in a framework of your choice to detect the cyclists.  "
      ],
      "metadata": {
        "id": "lyqaUbFA7Gbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = Roboflow(api_key=userdata.get('roboflow_key'))\n",
        "project = rf.workspace(\"test-yuzee\").project(\"visdronedi_mv\")\n",
        "version = project.version(3)\n",
        "dataset = version.download(\"yolov8\")"
      ],
      "metadata": {
        "id": "Z3CtYSzYOVaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo task=detect mode=train model=yolov8x.pt data=/content/VisDroneDI_MV-3/data.yaml epochs=20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qH91CONTRuhu",
        "outputId": "cb767c1f-0be5-4834-c42a-d105b37663a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.1.34 🚀 Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8x.pt, data=/content/VisDroneDI_MV-3/data.yaml, epochs=20, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train2\n",
            "2024-03-26 15:08:20.943820: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-26 15:08:20.943878: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-26 15:08:20.945706: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Overriding model.yaml nc=80 with nc=5\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      2320  ultralytics.nn.modules.conv.Conv             [3, 80, 3, 2]                 \n",
            "  1                  -1  1    115520  ultralytics.nn.modules.conv.Conv             [80, 160, 3, 2]               \n",
            "  2                  -1  3    436800  ultralytics.nn.modules.block.C2f             [160, 160, 3, True]           \n",
            "  3                  -1  1    461440  ultralytics.nn.modules.conv.Conv             [160, 320, 3, 2]              \n",
            "  4                  -1  6   3281920  ultralytics.nn.modules.block.C2f             [320, 320, 6, True]           \n",
            "  5                  -1  1   1844480  ultralytics.nn.modules.conv.Conv             [320, 640, 3, 2]              \n",
            "  6                  -1  6  13117440  ultralytics.nn.modules.block.C2f             [640, 640, 6, True]           \n",
            "  7                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n",
            "  8                  -1  3   6969600  ultralytics.nn.modules.block.C2f             [640, 640, 3, True]           \n",
            "  9                  -1  1   1025920  ultralytics.nn.modules.block.SPPF            [640, 640, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  3   1948800  ultralytics.nn.modules.block.C2f             [960, 320, 3]                 \n",
            " 16                  -1  1    922240  ultralytics.nn.modules.conv.Conv             [320, 320, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  3   7174400  ultralytics.nn.modules.block.C2f             [960, 640, 3]                 \n",
            " 19                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n",
            " 22        [15, 18, 21]  1   8722783  ultralytics.nn.modules.head.Detect           [5, [320, 640, 640]]          \n",
            "Model summary: 365 layers, 68157423 parameters, 68157407 gradients, 258.1 GFLOPs\n",
            "\n",
            "Transferred 589/595 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train2', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/VisDroneDI_MV-3/train/labels.cache... 4923 images, 0 backgrounds, 0 corrupt: 100% 4923/4923 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/VisDroneDI_MV-3/train/images/0000134_01791_d_0000147_jpg.rf.9cd939ae6846aae9e3f5030d0081f0bf.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/VisDroneDI_MV-3/train/images/0000137_02220_d_0000163_jpg.rf.ece1afa318ac995aa28ee45dcd8a0dc1.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/VisDroneDI_MV-3/train/images/0000140_00118_d_0000002_jpg.rf.f0a20cab3007409f185f40abb1e22387.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/VisDroneDI_MV-3/train/images/9999987_00000_d_0000049_jpg.rf.e73b51d4ee097fd1244bc113adecfa89.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/VisDroneDI_MV-3/valid/labels.cache... 1312 images, 0 backgrounds, 0 corrupt: 100% 1312/1312 [00:00<?, ?it/s]\n",
            "Plotting labels to runs/detect/train2/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 97 weight(decay=0.0), 104 weight(decay=0.0005), 103 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train2\u001b[0m\n",
            "Starting training for 20 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       1/20      15.4G      1.443      1.209      0.965        804        640: 100% 308/308 [07:21<00:00,  1.43s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 41/41 [00:33<00:00,  1.23it/s]\n",
            "                   all       1312      61767      0.425      0.298      0.284      0.166\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       2/20      14.9G       1.48      1.062     0.9706        975        640: 100% 308/308 [07:02<00:00,  1.37s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 41/41 [00:33<00:00,  1.23it/s]\n",
            "                   all       1312      61767      0.465      0.305      0.301      0.175\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       3/20      14.9G      1.468      1.045     0.9712        583        640: 100% 308/308 [07:00<00:00,  1.37s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 41/41 [00:32<00:00,  1.27it/s]\n",
            "                   all       1312      61767      0.446       0.33      0.325      0.194\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       4/20      15.2G      1.444      1.011     0.9597        820        640:  56% 173/308 [03:57<03:05,  1.37s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/yolo\", line 8, in <module>\n",
            "    sys.exit(entrypoint())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/cfg/__init__.py\", line 582, in entrypoint\n",
            "    getattr(model, mode)(**overrides)  # default args from model\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/engine/model.py\", line 657, in train\n",
            "    self.trainer.train()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/engine/trainer.py\", line 213, in train\n",
            "    self._do_train(world_size)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/engine/trainer.py\", line 381, in _do_train\n",
            "    self.loss, self.loss_items = self.model(batch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/nn/tasks.py\", line 88, in forward\n",
            "    return self.loss(x, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/nn/tasks.py\", line 267, in loss\n",
            "    return self.criterion(preds, batch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/utils/loss.py\", line 221, in __call__\n",
            "    _, target_bboxes, target_scores, fg_mask, _ = self.assigner(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/utils/tal.py\", line 72, in forward\n",
            "    mask_pos, align_metric, overlaps = self.get_pos_mask(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/utils/tal.py\", line 92, in get_pos_mask\n",
            "    mask_in_gts = self.select_candidates_in_gts(anc_points, gt_bboxes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/utils/tal.py\", line 227, in select_candidates_in_gts\n",
            "    bbox_deltas = torch.cat((xy_centers[None] - lt, rb - xy_centers[None]), dim=2).view(bs, n_boxes, n_anchors, -1)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.52 GiB. GPU 0 has a total capacity of 14.75 GiB of which 1.33 GiB is free. Process 196839 has 13.42 GiB memory in use. Of the allocated memory 12.65 GiB is allocated by PyTorch, and 555.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Kalman Filter (50 points)\n",
        "\n",
        "Use the  [filterpy](https://filterpy.readthedocs.io/en/latest/kalman/KalmanFilter.html) library to implement Kalman filters that will track the cyclist and the vehicle (if present) in the video. You will need to use the detections from the previous task to initialize and run the Kalman filter.\n",
        "\n",
        "You need to deliver a video that contains the trajectory of the objects as a line that connects the pixels that the tracker indicated. You can use the `ffmpeg` command line tool and OpenCV to superpose the bounding box of the drone on the video as well as plot its trajectory.\n",
        "\n",
        "Suggest methods that you can use to address  false positives and how the tracker can help you in this regard.\n",
        "\n",
        "You will need to have one Kalman filter to track each of the required and present objects (cyclist and vehicle)."
      ],
      "metadata": {
        "id": "zsJ2tfI-7Gxu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra Bonus (20 points)\n",
        "\n",
        "```{eval-rst}\n",
        ".. youtube:: 2hQx48U1L-Y\n",
        "```\n",
        "\n",
        "The cyclist in the video goes in and out of occlusions. In addition the object is small making detections fairly problematic without finetuning and other optimizations.  Fintetuning involves using the pretrained model and training it further using images of cyclists from a training dataset such as [VisDrone](https://github.com/VisDrone/VisDrone-Dataset). At the same time,  reducing the number of classes to a much smaller number such as person & bicycle may help.  Also some 2 stage detectors may need to be further optimized in terms of parameters for small objects. See [this paper](https://www.mdpi.com/1424-8220/23/15/6887) for ideas around small object tracking.\n",
        "\n",
        "\n",
        "```{note}\n",
        "The extra points can only be awarded in the category of `assignments` and cannot be used to compensate for any other category such as `exams`.\n",
        "```"
      ],
      "metadata": {
        "id": "Z9LQie817Pl9"
      }
    }
  ]
}