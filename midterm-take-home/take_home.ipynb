{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNPQeXEGsnDk"
      },
      "source": [
        "# CS-GY-66143 - MIDTERM EXAM SPRING 2024\n",
        "\n",
        "Please upload this problem in Brightspace together with your answers to the in-person portion. You need to submit your github URL and include this notebook with your answers for 1A and 1C in the same folder that you have answered 1B.  \n",
        "\n",
        "## PROBLEM 1: A JOURNEY THROUGH INFORMATION THEORY AND NEURAL NETWORKS (30 POINTS)\n",
        "\n",
        "### Introduction\n",
        "\n",
        "Information theory was introduced by Claude Shannon in 1948. It is a mathematical theory that deals with the transmission, processing, utilization, and extraction of information. It has given rise to a wide range of applications, including data compression, cryptography, error correction and fueled other industries such as AI, cellular communications and others.\n",
        "\n",
        "In this problem, we will use key concepts from information theory aiming in opening up the internals of neural networks and potentially explain how statistical learning theory and information theory can explain their behavior.\n",
        "\n",
        "The journey begins with [this reference](https://arxiv.org/abs/2206.07867) that you need to study before attempting the problem below.\n",
        "\n",
        "### PS 1A: Information Theory Basics (5 points)\n",
        "\n",
        "Let (x, y) have the following joint distribution:\n",
        "\n",
        "![](joint-distribution.png)\n",
        "\n",
        "If H is the symbol for the entropy functional, answer quantitatively showing your calculations\n",
        "\n",
        "a.  if H(x|y)=H(y|x) and if H(x) - H(x|y) =H(y)-H(y|x).\n",
        "\n",
        "b. Calculate the mutual information I(x,y)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Solution Part A:\n",
        "\n",
        "$$H(x \\mid y) = −\\sum_{i, j}​P(xi​,yj​)⋅\\log_2​(P(x_i \\mid y_j​)$$\n",
        "\n",
        "and\n",
        "\n",
        "$$ P(x|y) = \\frac{P(x, y)}{P(y)} $$\n",
        "\n",
        "and\n",
        "\n",
        "$$H(y \\mid x) = −\\sum_{i, j}​P(x_i​,y_j​)⋅\\log_2​(P(y_j​,x_i​)P(x_i​)$$\n",
        "\n",
        "and\n",
        "\n",
        "$$H(x) = −\\sum_{i}​P(x_i​​)⋅\\log_2​P(x_i​)$$"
      ],
      "metadata": {
        "id": "NZ2sSa-V7BfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import log2\n",
        "\n",
        "def calculate_conditional_entropy(joint_probs, marginal_probs):\n",
        "    conditional_entropy = 0\n",
        "\n",
        "    for y in range(len(joint_probs[0])):\n",
        "        for x in range(len(joint_probs)):\n",
        "            joint_prob = joint_probs[x][y]\n",
        "            if joint_prob > 0:  # Exclude cases where joint probability is zero\n",
        "                conditional_prob = joint_prob / marginal_probs[y]\n",
        "                conditional_entropy += joint_prob * log2(conditional_prob)\n",
        "\n",
        "    return -conditional_entropy\n",
        "\n",
        "def calculate_entropy(marginal_probs):\n",
        "    entropy = 0\n",
        "    for prob in marginal_probs:\n",
        "        if prob > 0:  # Exclude cases where probability is zero\n",
        "            entropy += prob * log2(prob)\n",
        "    return -entropy\n",
        "\n",
        "joint_probs = [ # copied from the given joint distribution table\n",
        "    [1/8, 1/16, 1/32, 1/32],\n",
        "    [1/16, 1/8, 1/32, 1/32],\n",
        "    [1/16, 1/16, 1/16, 1/16],\n",
        "    [1/4, 0, 0, 0]\n",
        "]\n",
        "\n",
        "marginal_probs_y = [1/4, 1/4, 1/4, 1/4] # manually calculated as the sum of each row\n",
        "marginal_probs_x = [1/2, 1/4, 1/8, 1/8] # manually calculated as the sum of each column\n",
        "\n",
        "conditional_entropy_given_y = calculate_conditional_entropy(joint_probs, marginal_probs_y)\n",
        "conditional_entropy_given_x = calculate_conditional_entropy(joint_probs, marginal_probs_x)\n",
        "\n",
        "entropy_y = calculate_entropy(marginal_probs_y)\n",
        "entropy_x = calculate_entropy(marginal_probs_x)\n",
        "\n",
        "print(\"H(x|y) =\", conditional_entropy_given_y)\n",
        "print(\"H(y|x) =\", conditional_entropy_given_x)\n",
        "print()\n",
        "print(\"H(x) - H(x|y) = \", entropy_x - conditional_entropy_given_y)\n",
        "print(\"H(y) - H(y|x) = \", entropy_y - conditional_entropy_given_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nQvWVTw_T79",
        "outputId": "1ea60f06-d68b-489d-b58b-8df38aed1773"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "H(x|y) = 1.375\n",
            "H(y|x) = 1.625\n",
            "\n",
            "H(x) - H(x|y) =  0.375\n",
            "H(y) - H(y|x) =  0.375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Summary\n",
        "\n",
        "Given the above calculations, we can conclude that $H(x \\mid y)$ does not equal $H(y \\mid x)$, but that $H(x) - H(x \\mid y)$ and $H(y) - H(y \\mid x)$ are indeed equivalent. This is not surprising given that both $H(x) - H(x \\mid y)$ and $H(y) - H(y \\mid x)$, as well as $H(X) + H(Y) − H(X, Y)$, are all different forms of defining mutual information, $I(x; y)$, as stated on the bottom of page 14 of [A Visual Introduction to Information Theory](https://arxiv.org/pdf/2206.07867.pdf)"
      ],
      "metadata": {
        "id": "XPnhw8fEBd6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Solution Part B:\n",
        "\n",
        "As mentioned in the Summary of part A,\n",
        "\n",
        "$$I(x; y) = H(x) - H(x \\mid y) = H(y) - H(y \\mid x)$$\n",
        "\n",
        "As we have already calculated $H(x) - H(x \\mid y) = H(y) - H(y \\mid x) = 0.375$, we know that:\n",
        "\n",
        " $$I(x; y) = 0.375$$"
      ],
      "metadata": {
        "id": "EPUsXYpVAdrR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocdsas0GsnDn"
      },
      "source": [
        "### PS 1B: Tishby's Information Bottleneck (20 points)\n",
        "\n",
        "In this seminal [paper](https://arxiv.org/abs/1703.00810), Tishby et al. propose a new framework for understanding the learning dynamics of deep neural networks. They argue that the learning process can be understood as an information bottleneck. This is [very nice video summary](https://www.youtube.com/watch?v=bLqJHjXihK8) of the key findings.\n",
        "\n",
        "Clone this repo https://github.com/pantelis/IDNNs and using the VSCode remote container extension, open the cloned repo in the tensorflow container. Please note the Dockerfile specification under the /docker folder - you may need to change the Dockerfile to use a CPU, for example, base image depending on your environment.\n",
        "\n",
        "a. Run the code in the repo and plot the training process on the information plane such as in the video and the figure below.\n",
        "\n",
        "![](information-plane.jpg)\n",
        "\n",
        "b. Add a folder `midterm-take-home` in your existing assignments repo.  Design a CNN classification network that can classify [the CIFAR10 dataset](https://huggingface.co/datasets/cifar10)  and introduce the information measures that the IDNN repo has introduced to produce the information plane and gradient mean and variance figures.\n",
        "\n",
        "Note that IDNN repo is based on the old version of tensorflow and you need to update the code to the latest version of tensorflow or pytorch. Both frameworks they need to be in their latest (but stable) version. Luckily a good chunk of supporting functions are framework independent.\n",
        "\n",
        "The new code should be in script *.py files but a tutorial notebook must also exist that links to the functions of the scripts and using markdown cells explains what the code is trying to do. Do not hesitate to use latex formulas to explain the concepts. Your focus in this tutorial treatment is on computational aspects and not on the information theoretic bounds outlined at some point in the video.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uusWnWS9snDo"
      },
      "source": [
        "### PS 1C: Commentary (5 points)\n",
        "\n",
        "Write a commentary on the findings of your own experiment (1B) highlighting the key findings in a similar way that Tishby et al. have done in their paper / video.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqGbFlA4snDo"
      },
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}